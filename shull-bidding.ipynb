{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <h1><center> Data Preparation and Machine Learning </center></h1>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import cross_val_score     \nfrom sklearn.metrics import confusion_matrix             \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression                     \nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom IPython.display import Image\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:53.703936Z","iopub.execute_input":"2022-01-22T12:13:53.704413Z","iopub.status.idle":"2022-01-22T12:13:53.714786Z","shell.execute_reply.started":"2022-01-22T12:13:53.70438Z","shell.execute_reply":"2022-01-22T12:13:53.713502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Online retailer, eBay is providing an option of bidding to their customers globally. Bidding is employed to find the real price of items in the market based on the demand. The price offered by anyone participating in this process is termed as a 'bid'. Normal bids are classified as ‘0’ bids in the data set and anomalous bids as ‘1’. The goal is to use classification or clustering algorithms to predict the bids in the future.","metadata":{}},{"cell_type":"markdown","source":"**Data Dictionary**\n* **Record ID**: Unique identifier of a record in the dataset.\n* **Auction ID**: Unique identifier of an auction.\n* **Bidder ID**: Unique identifier of a bidder.\n* **Bidder Tendency**: A shill bidder participates exclusively in auctions of few sellers rather than a diversified lot. This is a collusive act involving the fraudulent seller and an accomplice.\n* **Bidding Ratio**: A shill bidder participates more frequently to raise the auction price and attract higher bids from legitimate participants.\n* **Successive Outbidding**: A shill bidder successively outbids himself even though he is the current winner to increase the price gradually with small consecutive increments.\n* **Last Bidding**: A shill bidder becomes inactive at the last stage of the auction (more than 90\\% of the auction duration) to avoid winning the auction.\n* **Auction Bids**: Auctions with SB activities tend to have a much higher number of bids than the average of bids in concurrent auctions.\n* **Auction Starting Price**: a shill bidder usually offers a small starting price to attract legitimate bidders into the auction.\n* **Early Bidding**: A shill bidder tends to bid pretty early in the auction (less than 25\\% of the auction duration) to get the attention of auction users.\n* **Winning Ratio**: A shill bidder competes in many auctions but hardly wins any auctions.\n* **Auction Duration**: How long an auction lasted.\n* **Class**: 0 for normal behaviour bidding; 1 for otherwise.","metadata":{}},{"cell_type":"markdown","source":"## Content:\n### 1. Bussiness Understanding;\n### 2. Data Understanding;\n### 3. Data preparation;\n### 4. Modeling;\n### 5. Evaluation;\n### 6. Deployment;\n### References.","metadata":{}},{"cell_type":"markdown","source":"For purpose of this project we are going to use established model for data mining - **CRISP-DM** (the cross-industry standard process for data mining). CRISP-DM project consists of 6 stages. Following one another, it helps in keeping sequence of project's steps. The defined stages of CRISP-DM are:\n* Business Understanding;\n* Data understanding;\n* Data Preparation;\n* Modeling;\n* Evaluation;\n* Deployment.","metadata":{}},{"cell_type":"markdown","source":"### 1. Business understanding","metadata":{}},{"cell_type":"markdown","source":"The purpose of the Business Understanding phase is to understand what the business wants to solve. On this stage we set questions we need to answer with the project's outcome. Following the assigment task we can define the next questions within the project:\n* What features are the most important for bidding class prediction?\n* Can a target variable be predicted with an accuracy higher than 90%?","metadata":{}},{"cell_type":"markdown","source":"### 2. Data Understanding","metadata":{}},{"cell_type":"markdown","source":"The data understanding phase goes hand in hand with the business understanding phase and encourages the focus to ascertain, assemble, and scrutinize the data sets that can help us to achieve the project goals. (Trisit Kumar Chatterjee, 2020)","metadata":{}},{"cell_type":"markdown","source":"My first step will be uploading the dataset. Also, I want to set **Record_ID** as an index column. The reason for that is that the **Record_ID** column contains a unique identification number which can facilitate data retrieval and handling data in general.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/shill-bidding-dataset/Shill Bidding Dataset.csv', index_col='Record_ID')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:53.716764Z","iopub.execute_input":"2022-01-22T12:13:53.717275Z","iopub.status.idle":"2022-01-22T12:13:53.748804Z","shell.execute_reply.started":"2022-01-22T12:13:53.717228Z","shell.execute_reply":"2022-01-22T12:13:53.747923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:53.75004Z","iopub.execute_input":"2022-01-22T12:13:53.750408Z","iopub.status.idle":"2022-01-22T12:13:53.771391Z","shell.execute_reply.started":"2022-01-22T12:13:53.750365Z","shell.execute_reply":"2022-01-22T12:13:53.7705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I want to see if we have any missing values and what type of the observations we have. I will use ***.info*** function.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:53.772287Z","iopub.execute_input":"2022-01-22T12:13:53.772518Z","iopub.status.idle":"2022-01-22T12:13:53.792957Z","shell.execute_reply.started":"2022-01-22T12:13:53.772489Z","shell.execute_reply":"2022-01-22T12:13:53.791955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that overall the dataset consists of numerical data with one column represented by 'object' data type. Data size is 6321 rows and 12 columns. We also don't observe any missing values, which means we won't have to handle them in the next chapter. \n\nNext, I want to extract some statistical information and then get a visual understanding of the data and plot some graphs.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-22T12:13:53.794911Z","iopub.execute_input":"2022-01-22T12:13:53.795136Z","iopub.status.idle":"2022-01-22T12:13:53.832803Z","shell.execute_reply.started":"2022-01-22T12:13:53.795108Z","shell.execute_reply":"2022-01-22T12:13:53.832184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table above we can observe, that values range from 0 to 1 for each feature except **Auction_Duration** and **Auction_ID**. **Last_Bidding** and **Auction_Duration** seem to follow a normal distribution. Also, I can infer that **Winning_Ratio** and **Successive_Outbidding** have lots of observations cluster around either **0** or **1** values. And the dependent variable **Class** is more represented by **0** class than **1** as mean equals 0.1. It means that the data is imbalanced and we will have to decide how to deal with it in the next chapter. \nLet's see how many observations we have represented for each class.","metadata":{}},{"cell_type":"code","source":"sns.countplot(df['Class'])\nprint(plt.show())\nprint(df.Class.value_counts())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-22T12:13:53.833546Z","iopub.execute_input":"2022-01-22T12:13:53.833747Z","iopub.status.idle":"2022-01-22T12:13:53.936386Z","shell.execute_reply.started":"2022-01-22T12:13:53.833723Z","shell.execute_reply":"2022-01-22T12:13:53.935526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the graph as well as with the ***value_counts*** we can confirm that **0** class has 5646 entries while **1** class has only 675 entries. I will see models' accuracy in the Modeling chapter and if imbalanced classes strongly affect accuracy we will have to balance them.","metadata":{}},{"cell_type":"markdown","source":"As the only one object feature we have is a bidder name I want to see if I have any unique values and then decide if we need to keep the feature.","metadata":{}},{"cell_type":"code","source":"df.Bidder_ID.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:53.937741Z","iopub.execute_input":"2022-01-22T12:13:53.938032Z","iopub.status.idle":"2022-01-22T12:13:53.95183Z","shell.execute_reply.started":"2022-01-22T12:13:53.937991Z","shell.execute_reply":"2022-01-22T12:13:53.950625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that we have 1054 unique values. We can't really extract any usefull information from it as it's represented by encrypted unique names, therefore I'll simply drop it.","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns='Bidder_ID')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:53.953633Z","iopub.execute_input":"2022-01-22T12:13:53.95429Z","iopub.status.idle":"2022-01-22T12:13:53.961414Z","shell.execute_reply.started":"2022-01-22T12:13:53.954164Z","shell.execute_reply":"2022-01-22T12:13:53.96047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we are dealing with a classification problem we don't need to identify correlation between variables.","metadata":{}},{"cell_type":"markdown","source":"I also want to take a look at boxplots for each variable to get a better understanding of distribution and outliers.","metadata":{}},{"cell_type":"code","source":"df.drop('Class', axis=1).plot(kind='box', subplots=True, layout=(6,2), sharex=False, sharey=False, figsize=(10,10), title='Box Plot for each input variable')\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-22T12:13:53.963296Z","iopub.execute_input":"2022-01-22T12:13:53.964012Z","iopub.status.idle":"2022-01-22T12:13:54.728857Z","shell.execute_reply.started":"2022-01-22T12:13:53.963965Z","shell.execute_reply":"2022-01-22T12:13:54.727988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bidder_Tendency** and **Bidding_Ratio** have many outliers. We will discuss in the next chapter if we need to remove them. **Auction_ID**, **Last_Bidding**, **Early_Bidding** and **Auction_Duration** seem to follow normal distribution. We infered earlier that **Winning_Ratio** and **Starting_Price_Average** have values mostly clustered around **0** and **1**. **Successive_Outbidding** boxplot looks interesting, as it shows that we migh have outliers. I am going to plot density graphs for these 3 features to get a better visual comprehension.","metadata":{}},{"cell_type":"code","source":"print(sns.kdeplot(data=df, x='Winning_Ratio', color='r', shade=True))\nprint(sns.kdeplot(data=df, x='Starting_Price_Average', color='y', shade=True))\nprint(sns.kdeplot(data=df, x='Successive_Outbidding', color='b', shade=True))\n\nplt.xlabel('KDE')\nplt.ylabel('Probability Density')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:54.731927Z","iopub.execute_input":"2022-01-22T12:13:54.732184Z","iopub.status.idle":"2022-01-22T12:13:54.994814Z","shell.execute_reply.started":"2022-01-22T12:13:54.732154Z","shell.execute_reply":"2022-01-22T12:13:54.994075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the graph we can confirm the statement we made above. As for **Successive_Outbidding** variable, it's not ouliers but data is represented by only 3 values **0**, **0.5** and **1**. As number of **0** values are drastically higher than **0.5** and **1** it shows the values as outliers.","metadata":{}},{"cell_type":"code","source":"print(df.Successive_Outbidding.value_counts())\nprint(df.Successive_Outbidding.describe())\nprint(sns.boxplot(data=df, x='Successive_Outbidding'))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:54.997869Z","iopub.execute_input":"2022-01-22T12:13:54.998294Z","iopub.status.idle":"2022-01-22T12:13:55.126154Z","shell.execute_reply.started":"2022-01-22T12:13:54.998256Z","shell.execute_reply":"2022-01-22T12:13:55.125308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've prepared everything we needed and now can move to another stage **Data preparation**.","metadata":{}},{"cell_type":"markdown","source":"### 3. Data Preparation","metadata":{}},{"cell_type":"markdown","source":"This stage, which is often referred to as “data wrangling” or “data munging”, has the objective is to develop the final data set for modelling. Covers all activities to construct the final dataset from the initial raw data. Data preparation tasks are likely to be performed multiple times and not in any prescribed order. Tasks include table, record and attribute selection as well as transformation and cleaning of data for modelling tools.","metadata":{}},{"cell_type":"markdown","source":"I'll start the Data Preparation section with normalization. Normalization typically means rescaling the values into a range of [0,1]. (Geller, 2019)\nWhen we have all features normalized within one range it will help ML models run faster and with the better accuracy.\nThe reason why I'll be using normalization is that most of the features are already ranged between 0 and 1 except **Auction_ID** and **Auction_Duration**, therefore I will scale only these two features.","metadata":{}},{"cell_type":"code","source":"df1 = df[['Auction_ID','Auction_Duration']]\ndf2 = df.drop(columns = ['Auction_ID','Auction_Duration'])\nminmax = MinMaxScaler().fit(df1)\nminmax = minmax.transform(df1)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.128027Z","iopub.execute_input":"2022-01-22T12:13:55.128486Z","iopub.status.idle":"2022-01-22T12:13:55.138696Z","shell.execute_reply.started":"2022-01-22T12:13:55.128442Z","shell.execute_reply":"2022-01-22T12:13:55.137899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After I've applied the scaler I want to record the scaled array in DataFrame and then combine in one DataFrame.","metadata":{}},{"cell_type":"code","source":"df2 = df2.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.140148Z","iopub.execute_input":"2022-01-22T12:13:55.140764Z","iopub.status.idle":"2022-01-22T12:13:55.155889Z","shell.execute_reply.started":"2022-01-22T12:13:55.140718Z","shell.execute_reply":"2022-01-22T12:13:55.155113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(minmax, columns = ['Auction_ID','Auction_Duration'])\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.157604Z","iopub.execute_input":"2022-01-22T12:13:55.15826Z","iopub.status.idle":"2022-01-22T12:13:55.175223Z","shell.execute_reply.started":"2022-01-22T12:13:55.158211Z","shell.execute_reply":"2022-01-22T12:13:55.174134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df1.join(df2, how='outer')\nindex = df2['Record_ID']\ndf = df.set_index('Record_ID')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.176442Z","iopub.execute_input":"2022-01-22T12:13:55.176949Z","iopub.status.idle":"2022-01-22T12:13:55.203517Z","shell.execute_reply.started":"2022-01-22T12:13:55.176904Z","shell.execute_reply":"2022-01-22T12:13:55.202709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we've got the DataFrame with all feature ranged with the same range [0,1]. ","metadata":{}},{"cell_type":"markdown","source":"#### Features importances","metadata":{}},{"cell_type":"markdown","source":"As a part of my business understanding part I've defined a question I would like to get an answer *'What features are the most important for bidding class prediction?'*\nIn order to get an answer on the question I'll be using the ***DecisionTreeClassifier*** which will range fetures by their importance.\nFeature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature. (Ronaghan, 2018)","metadata":{}},{"cell_type":"markdown","source":"As a first step of defining features importances I'll separate independent variables from the dependent variable.","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns = 'Class')\ny = df['Class']","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.204655Z","iopub.execute_input":"2022-01-22T12:13:55.204924Z","iopub.status.idle":"2022-01-22T12:13:55.210192Z","shell.execute_reply.started":"2022-01-22T12:13:55.204893Z","shell.execute_reply":"2022-01-22T12:13:55.209263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I'll run the DecisionTreeClassifier.","metadata":{}},{"cell_type":"code","source":"cls = DecisionTreeClassifier()\ncls.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.211367Z","iopub.execute_input":"2022-01-22T12:13:55.21173Z","iopub.status.idle":"2022-01-22T12:13:55.237776Z","shell.execute_reply.started":"2022-01-22T12:13:55.211688Z","shell.execute_reply":"2022-01-22T12:13:55.236726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = cls.feature_importances_\ncolumns = X.columns\ncolumns","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.239105Z","iopub.execute_input":"2022-01-22T12:13:55.239319Z","iopub.status.idle":"2022-01-22T12:13:55.245687Z","shell.execute_reply.started":"2022-01-22T12:13:55.239293Z","shell.execute_reply":"2022-01-22T12:13:55.244658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to plot a graph to display features importances in descending order.","metadata":{}},{"cell_type":"code","source":"df_import = pd.DataFrame(importances, columns = ['Feature importances'], index=columns)\ndf_import['Percentage'] = pd.DataFrame((df_import.sort_values(by= ['Feature importances'], ascending=False)/df_import.shape[0])*100)\npd.options.display.float_format = \"{:.2f}\".format \ndf_import=df_import.sort_values(by= ['Percentage'], ascending=False)\n\n# plotting features importances\ng = df_import['Percentage'].plot(kind = 'bar', figsize=(60,30), fontsize=24, color = 'pink')\ng.set_title('Features importances', fontsize=70)\ng.set_xlabel('Features', fontsize=50)\ng.set_ylabel('Percentage of importances', fontsize=50)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:55.247156Z","iopub.execute_input":"2022-01-22T12:13:55.24764Z","iopub.status.idle":"2022-01-22T12:13:56.001521Z","shell.execute_reply.started":"2022-01-22T12:13:55.247606Z","shell.execute_reply":"2022-01-22T12:13:56.000552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that three the most important features for predicting the target variable are:\n* ***Successive_Outbidding***;\n* ***Auction_Duration***;\n* ***Winning_Ratio***,\n\nand the less important are:\n* ***Starting_Price_Average***;\n* ***Auction_ID***;\n* ***Early_Bidding***.","metadata":{}},{"cell_type":"markdown","source":"#### Dimensionality reduction","metadata":{}},{"cell_type":"markdown","source":"We use dimensionality reduction when  we want to decrease the number of features in a dataset. Two techniques for dimensionality reduction are:\n* Principal Component Analysis (PCA);\n* Linear Discriminant Analysis (LDA).\n\nBoth LDA and PCA are linear transformation techniques: LDA is a supervised whereas PCA is unsupervised – PCA ignores class labels. In contrast to PCA, LDA attempts to find a feature subspace that maximizes class separability. (Raschka, 2022)\nIn this chapter I want to apply both PCA and LDA and compare results.","metadata":{}},{"cell_type":"markdown","source":"##### PCA","metadata":{}},{"cell_type":"markdown","source":"PCA is a statistical variance based procedure that converts a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. In simpler words, PCA is often used to simplify data, reduce noise, and find unmeasured “latent variables”. (Volpi, 2020)\nWe've already separated independants variables and the target variable, therefore we can apply PCA straight away.","metadata":{}},{"cell_type":"markdown","source":"First, I want to plot an Explained varaince ratio graph to see how much of the variance in the original data is encapsulated in the new component variables.","metadata":{}},{"cell_type":"code","source":"pca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title('Explained variance ratio', fontsize = 20)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.00279Z","iopub.execute_input":"2022-01-22T12:13:56.003026Z","iopub.status.idle":"2022-01-22T12:13:56.235023Z","shell.execute_reply.started":"2022-01-22T12:13:56.002996Z","shell.execute_reply":"2022-01-22T12:13:56.234182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph above we see that in order to keep 90% of the original data we need to use 6 components.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=6)\nprojected = pca.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.236617Z","iopub.execute_input":"2022-01-22T12:13:56.236929Z","iopub.status.idle":"2022-01-22T12:13:56.257499Z","shell.execute_reply.started":"2022-01-22T12:13:56.236889Z","shell.execute_reply":"2022-01-22T12:13:56.256781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explained_variance = pca.explained_variance_ratio_\nexplained_variance","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.25882Z","iopub.execute_input":"2022-01-22T12:13:56.259132Z","iopub.status.idle":"2022-01-22T12:13:56.265917Z","shell.execute_reply.started":"2022-01-22T12:13:56.259092Z","shell.execute_reply":"2022-01-22T12:13:56.265115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First 3 components contain the most ammount of data while the last three contain less than 20%. This the last step I want to create a DataFrame with 6 components and a target variable.","metadata":{}},{"cell_type":"code","source":"principal_df = pd.DataFrame(data = projected, columns = ['PC 1','PC 2','PC 3','PC 4','PC 5','PC 6'], index=index)\ndf_pca = principal_df.join(y, how='outer')\ndf_pca.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.267342Z","iopub.execute_input":"2022-01-22T12:13:56.268125Z","iopub.status.idle":"2022-01-22T12:13:56.304625Z","shell.execute_reply.started":"2022-01-22T12:13:56.268079Z","shell.execute_reply":"2022-01-22T12:13:56.303644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### LDA","metadata":{}},{"cell_type":"markdown","source":"Linear Discriminant Analysis, or LDA, uses the information from both features to create a new axis and projects the data on to the new axis in such a way as to minimizes the variance and maximizes the distance between the means of the two classes. (Maklin, 2019)","metadata":{}},{"cell_type":"markdown","source":"I start with ***fit_transform*** of the both dependent and independent variables and then will take a look at the explained_variance_ratio.","metadata":{}},{"cell_type":"code","source":"lda = LinearDiscriminantAnalysis()\nx_lda = lda.fit_transform(X,y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.31109Z","iopub.execute_input":"2022-01-22T12:13:56.314028Z","iopub.status.idle":"2022-01-22T12:13:56.346546Z","shell.execute_reply.started":"2022-01-22T12:13:56.313951Z","shell.execute_reply":"2022-01-22T12:13:56.345594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.355734Z","iopub.execute_input":"2022-01-22T12:13:56.358771Z","iopub.status.idle":"2022-01-22T12:13:56.371808Z","shell.execute_reply.started":"2022-01-22T12:13:56.358697Z","shell.execute_reply":"2022-01-22T12:13:56.370687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our target variable is represented only by 2 classes, the LDA has reduced the number of components to 1. If we had at least two components we would be able to plot the data on the two-dimension graph. However in our case the data is represented by one dimension and the plot would look like a simple one dimension line.\nI want to create a DataFrame and record there LDA 1 we got as well as the target variable.","metadata":{}},{"cell_type":"code","source":"linear_df = pd.DataFrame(data = x_lda, columns = ['LDA 1'], index=index)\ndf_lda = linear_df.join(y, how='outer')\ndf_lda.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.378163Z","iopub.execute_input":"2022-01-22T12:13:56.381202Z","iopub.status.idle":"2022-01-22T12:13:56.405038Z","shell.execute_reply.started":"2022-01-22T12:13:56.381134Z","shell.execute_reply":"2022-01-22T12:13:56.404057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table above we see, that negative numbers in LDA 1 belong to '0 class' and positive numbers belong to '1 class'.","metadata":{}},{"cell_type":"markdown","source":"PCA dimensionality reduction method allows us to choose the number of components we need to use. Apart from PCA in LDA we can't control the number of components they will be always reduced by one from number of classes we have represented as a target variable. However, LDA is a better use for solving classification problems as it creates a clear separation between classes. ","metadata":{}},{"cell_type":"markdown","source":"It was the last step of this stage. The dataset is ready for applying ML models. For the purpose of the modeling stage I'm going to use dataset with applied LDA.","metadata":{}},{"cell_type":"markdown","source":"### 4. Modeling","metadata":{}},{"cell_type":"markdown","source":"At this stage we will select the actual modelling technique that is to be used. It can be one or several models. According to the CA task I'll choose two models to work with and will explain my choice.\nAs a first step of the stage I'll separate the target variable from dependent variables.","metadata":{}},{"cell_type":"code","source":"X = df_lda.drop(columns='Class')\ny = df_lda['Class']","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.410894Z","iopub.execute_input":"2022-01-22T12:13:56.413654Z","iopub.status.idle":"2022-01-22T12:13:56.422955Z","shell.execute_reply.started":"2022-01-22T12:13:56.41358Z","shell.execute_reply":"2022-01-22T12:13:56.421989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I'll split the dataset on train and test parts, keeping train part equals to 65% and train part 35%.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=1, shuffle=True)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.429295Z","iopub.execute_input":"2022-01-22T12:13:56.430938Z","iopub.status.idle":"2022-01-22T12:13:56.444342Z","shell.execute_reply.started":"2022-01-22T12:13:56.43086Z","shell.execute_reply":"2022-01-22T12:13:56.44336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first algorithm I'll be working with is a Logistic Regression. Logistic Regression is a Supervised Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. (Li, 2017).\nThe reason why I'll be using the Logistic regression algorithm is because it's designed for two-class (binary) problems, modeling the target using a binomial probability distribution function.","metadata":{}},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"First step is to fit the model:","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.446062Z","iopub.execute_input":"2022-01-22T12:13:56.44698Z","iopub.status.idle":"2022-01-22T12:13:56.471547Z","shell.execute_reply.started":"2022-01-22T12:13:56.446932Z","shell.execute_reply":"2022-01-22T12:13:56.470535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the next step I want to define the best model's parameters in order to get the best possible accuracy.\nI'll be working with the next LogisticRegression parameters:\n* **solvers**;\n* **penalty**;\n* **c_values**.\n\n**Solvers** - Algorithm to use in the optimization problem:\n* For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n* For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n* ‘liblinear’ is limited to one-versus-rest schemes.\n\n**Penalty** - Specify the norm of the penalty:\n* 'none': no penalty is added;\n* 'l2': add a L2 penalty term and it is the default choice;\n* 'l1': add a L1 penalty term;\n\n**C_values** - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.","metadata":{}},{"cell_type":"markdown","source":"##### Tuning hyper-parameters of an estimator","metadata":{}},{"cell_type":"markdown","source":"We need to records parameters we've defined in a list:","metadata":{}},{"cell_type":"code","source":"solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\npenalty = ['none','l1','l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.475155Z","iopub.execute_input":"2022-01-22T12:13:56.475393Z","iopub.status.idle":"2022-01-22T12:13:56.479389Z","shell.execute_reply.started":"2022-01-22T12:13:56.475366Z","shell.execute_reply":"2022-01-22T12:13:56.478718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To tune the parameters we will be using GridSearchCV. GridSearchCV is as exhaustive search over specified parameter values for an estimator. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. (Shah, 2021)","metadata":{}},{"cell_type":"code","source":"grid = dict(solver=solvers,penalty=penalty,C=c_values)\ngrid_search = GridSearchCV(estimator=logreg, param_grid=grid, n_jobs=-1, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:56.480242Z","iopub.execute_input":"2022-01-22T12:13:56.480569Z","iopub.status.idle":"2022-01-22T12:13:58.442946Z","shell.execute_reply.started":"2022-01-22T12:13:56.480537Z","shell.execute_reply":"2022-01-22T12:13:58.442103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.444358Z","iopub.execute_input":"2022-01-22T12:13:58.444825Z","iopub.status.idle":"2022-01-22T12:13:58.450427Z","shell.execute_reply.started":"2022-01-22T12:13:58.444794Z","shell.execute_reply":"2022-01-22T12:13:58.449406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've defined that the best accuracy will'be reached by using C=0.1, penalty = 'l2' and a  solver = 'liblinear'. Now, let's train the model with these parameters:","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(C=0.1, penalty='l2', solver='liblinear')\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.453528Z","iopub.execute_input":"2022-01-22T12:13:58.454467Z","iopub.status.idle":"2022-01-22T12:13:58.471389Z","shell.execute_reply.started":"2022-01-22T12:13:58.45443Z","shell.execute_reply":"2022-01-22T12:13:58.470446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what accuracy we get on training and testins set to make sure the model is not underfitted or overfitted.","metadata":{}},{"cell_type":"code","source":"print('Training score: {:.3f}'.format(logreg.score(X_train,y_train)))\nprint('Testing score: {:.3f}'.format(logreg.score(X_test,y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.47242Z","iopub.execute_input":"2022-01-22T12:13:58.472853Z","iopub.status.idle":"2022-01-22T12:13:58.484543Z","shell.execute_reply.started":"2022-01-22T12:13:58.472818Z","shell.execute_reply":"2022-01-22T12:13:58.483889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wee see that both training and testing scores are high and the model perfoms well on both sets. The last step is to run the classification report and identify ***Type I and Type II errors***.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))\nprint(sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='.4g',                 \n                 xticklabels=['Normal behaviour (0 class)','Otherwise (1 class)'],                \n                 yticklabels=['Normal behaviour (0 class)','Otherwise (1 class)'],                  \n                 cbar=False, cmap='Blues'))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.485545Z","iopub.execute_input":"2022-01-22T12:13:58.486126Z","iopub.status.idle":"2022-01-22T12:13:58.625438Z","shell.execute_reply.started":"2022-01-22T12:13:58.486092Z","shell.execute_reply":"2022-01-22T12:13:58.624383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can obserse that the model perfomed very well. Over 1984 **0 class** values 55 were predicted as False Positive and over 224 **1 class** values only 1 was predicted as False Negative, which is a very good result.","metadata":{}},{"cell_type":"markdown","source":"Next ML model I want to use is a DecisionTreeClassifier. Decision Tree is a Supervised Machine Learning Algorithm that uses a set of rules to make decisions. The intuition behind Decision Trees is that we use the dataset features to create yes/no questions and continually split the dataset until we isolate all data points belonging to each class. Every time we ask a question we add a node to the tree. The result of asking a question splits the dataset based on the value of a feature, and creates new nodes. The algorithm tries to completely separate the dataset such that all leaf nodes, i.e., the nodes that don’t split the data further, belong to a single class. These are called pure leaf nodes. In the end, the algorithm can only assign one class to the data points in each leaf node. (Bento, 2021)","metadata":{}},{"cell_type":"markdown","source":"#### Decision Tree","metadata":{}},{"cell_type":"markdown","source":"Fitting the model first:","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier(random_state=1)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.627101Z","iopub.execute_input":"2022-01-22T12:13:58.628167Z","iopub.status.idle":"2022-01-22T12:13:58.644303Z","shell.execute_reply.started":"2022-01-22T12:13:58.62811Z","shell.execute_reply":"2022-01-22T12:13:58.643235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly to the previous chapter we want to define the best parameters in order to reach the best accuracy. For this model I'll be chechking the next parameters:\n* **Max_depth**;\n* **Min_samples_leaf**.\n\n**Max_depth** - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n**Min_samples_leaf** - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. ","metadata":{}},{"cell_type":"markdown","source":"##### Tuning hyper-parameters of an estimator","metadata":{}},{"cell_type":"markdown","source":"For both hyperparameters defined I'll create a range between 1 to 20. The estimator will check each value of both in order to define the best ones.","metadata":{}},{"cell_type":"code","source":"depth = range(1, 20)\nleaf = range(1, 20)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.645822Z","iopub.execute_input":"2022-01-22T12:13:58.646905Z","iopub.status.idle":"2022-01-22T12:13:58.652662Z","shell.execute_reply.started":"2022-01-22T12:13:58.646845Z","shell.execute_reply":"2022-01-22T12:13:58.651498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = dict(max_depth=depth,min_samples_leaf=leaf)\ngrid_search = GridSearchCV(estimator=tree, param_grid=grid, n_jobs=-1, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:13:58.654244Z","iopub.execute_input":"2022-01-22T12:13:58.654745Z","iopub.status.idle":"2022-01-22T12:14:03.931794Z","shell.execute_reply.started":"2022-01-22T12:13:58.654699Z","shell.execute_reply":"2022-01-22T12:14:03.930941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:14:03.935541Z","iopub.execute_input":"2022-01-22T12:14:03.936902Z","iopub.status.idle":"2022-01-22T12:14:03.942288Z","shell.execute_reply.started":"2022-01-22T12:14:03.936836Z","shell.execute_reply":"2022-01-22T12:14:03.941684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the estimator has defined **max_depth** and **min_samples_leaf** both equal 4. Let's train the model by using defined parameters.","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=4, random_state=1)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:14:03.943485Z","iopub.execute_input":"2022-01-22T12:14:03.943958Z","iopub.status.idle":"2022-01-22T12:14:03.967387Z","shell.execute_reply.started":"2022-01-22T12:14:03.943919Z","shell.execute_reply":"2022-01-22T12:14:03.966637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training score: {:.3f}'.format(tree.score(X_train,y_train)))\nprint('Testing score: {:.3f}'.format(tree.score(X_test,y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:14:03.969825Z","iopub.execute_input":"2022-01-22T12:14:03.970048Z","iopub.status.idle":"2022-01-22T12:14:03.987635Z","shell.execute_reply.started":"2022-01-22T12:14:03.970023Z","shell.execute_reply":"2022-01-22T12:14:03.986737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training and testing accuracy I've got are a bit higher than with the LogisticRegression and should give us a better result in the classification report.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))\nprint(sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='.4g',                 \n                 xticklabels=['Normal behaviour (0 class)','Otherwise (1 class)'],                \n                 yticklabels=['Normal behaviour (0 class)','Otherwise (1 class)'],                  \n                 cbar=False, cmap='Blues'))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:14:03.989076Z","iopub.execute_input":"2022-01-22T12:14:03.989932Z","iopub.status.idle":"2022-01-22T12:14:04.127462Z","shell.execute_reply.started":"2022-01-22T12:14:03.989882Z","shell.execute_reply":"2022-01-22T12:14:04.126492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We infer that with this model over 1984 **0 class** values 46 were predicted as False Positive and over 224 **1 class** values 2 were predicted as False Negative.","metadata":{}},{"cell_type":"markdown","source":"In the table below some main comparative characteristics are presented:\n\n\n| <center> **Comparative characteristics**  <center>   |  <center> **LogisticRegression** <center>  |  <center> **DecisionTreeClassifier**  <center> |  \n| ----------- | ----------- | ----------- |\n| <center> Number of used hyperparameters  <center>    | <center> 3  <center>     | <center> 2<center> | \n| <center> Running time <center>  | <center> ~ 5 sec <center>        | <center> ~ 5 sec <center> | \n| <center> Training accuracy  <center>  | <center> 0.979 <center>        | <center> 0.984 <center> | \n| <center> Testing accuracy  <center>  | <center> 0.975 <center>        | <center> 0.978 <center> | \n| <center> Type I error  <center>  | <center> 55 <center>        | <center> 46 <center> |\n| <center> Type II error  <center>  | <center> 1 <center>        | <center> 2 <center> |\n    \nWe can conclude, that overall both models perfomed very well, however the DecisionTreeClassifier perfomed slightly better.","metadata":{}},{"cell_type":"markdown","source":"### 5. Evaluation","metadata":{}},{"cell_type":"markdown","source":"The Evaluation phase is different from the Modeling technical evaluation. This phase evaluates the model concerning the business indicator and what to do next.\nOn the business understanding stage we've defined two main question we would like to get answers with the project:\n* What features are the most important for bidding class prediction?\n* Can a target variable be predicted with an accuracy higher than 90%?\n\nWe detected that three the most important features for predicting the target variable are:\n\n* **Successive_Outbidding**;\n* **Auction_Duration**;\n* **Winning_Ratio**,\n\nand the less important are:\n\n* **Starting_Price_Average**;\n* **Auction_ID**;\n* **Early_Bidding**.\n\nIt means that next time when mining data we can focus more on getting observations for top 3 most important features and don't give great importance the less important features.\n\nAlso we found out that the target variable can be predicted with an accuracy higher than 90% if using the LogisticRegression and the DecisionTreeClassifier models.","metadata":{}},{"cell_type":"markdown","source":"### 6. Deployment","metadata":{}},{"cell_type":"markdown","source":"Although on practice the stage is being used as a way to deliver findings and outcomes to the user, I'll use the part to conclude what I've done.\nSo far, I've been able to deliver business goals, I had defined on the business understanding stage. I determined most important features which can be used in future for data mining. Both of model had perfomed really well and satisfied the accuracy requirements. I tuned hyperparameters and compared perfomance of two models. I was able to implement two dimensionality reduction techniques and compared the results.","metadata":{}}]}